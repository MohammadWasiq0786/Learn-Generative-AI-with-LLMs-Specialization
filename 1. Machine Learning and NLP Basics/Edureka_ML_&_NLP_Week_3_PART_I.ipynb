{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing (NLP) Tutorial\n",
        "\n",
        "## Introduction to Natural Language Processing:\n",
        "\n",
        "Natural Language Processing (NLP) is a branch of artificial intelligence (AI) that focuses on the interaction between computers and human language. It involves the development of algorithms and models that enable computers to understand, interpret, and generate human language in a meaningful way.\n",
        "\n",
        "### History of NLP:\n",
        "\n",
        "* 1940-1960: The early years of NLP were primarily focused on machine translation (MT). Notable developments include the first recognizable NLP application at Birkbeck College, London, in 1948.\n",
        "* 1960-1980: This period saw the emergence of augmented transition networks (ATN), case grammar, and systems like SHRDLU and LUNAR, which laid the foundation for modern NLP.\n",
        "* 1980-Current: Post-1980, NLP witnessed advancements in machine learning algorithms, leading to improved accuracy and efficiency in language processing tasks. The growth of the internet and availability of large text corpora further accelerated progress in NLP.\n",
        "\n",
        "### Importance and Applications of NLP:\n",
        "\n",
        "NLP has diverse applications across various domains, including:\n",
        "\n",
        "* Information Retrieval: Extracting relevant information from large volumes of text data.\n",
        "* Sentiment Analysis: Analyzing and understanding the sentiment or emotion expressed in text.\n",
        "* Text Summarization: Generating concise summaries of lengthy documents or articles.\n",
        "* Machine Translation: Translating text from one language to another.\n",
        "* Speech Recognition: Converting spoken language into text.\n",
        "* Chatbots: Building conversational agents for customer support and interaction.\n",
        "* Named Entity Recognition (NER): Identifying and classifying named entities such as names, organizations, dates, and locations in text.\n",
        "\n",
        "### NLP Fundamentals:\n",
        "\n",
        "To understand NLP, it's essential to grasp some fundamental concepts:\n",
        "\n",
        "* Syntax: The structure of language, including grammar rules and sentence structure.\n",
        "* Semantics: The meaning of words, phrases, and sentences in context.\n",
        "* Pragmatics: The study of how language is used in real-world situations to convey meaning and achieve communication goals.\n",
        "* Tokenization: Breaking down text into smaller units such as words or sentences for analysis.\n",
        "* Stemming and Lemmatization: Techniques to reduce words to their root forms, improving consistency in text analysis.\n",
        "\n",
        "### Text Preprocessing in NLP:\n",
        "\n",
        "Text preprocessing is a critical step in NLP that involves cleaning and preparing text data for analysis. This may include:\n",
        "\n",
        "* Removing special characters, punctuation, and numerical values.\n",
        "* Converting text to lowercase to standardize the text format.\n",
        "* Removing stop words (commonly occurring words such as \"the,\" \"is,\" \"and\") that carry little semantic meaning.\n",
        "* Tokenization to split text into individual words or phrases.\n",
        "* Stemming and lemmatization to reduce words to their base or root forms.\n",
        "\n",
        "### Feature Extraction in NLP:\n",
        "\n",
        "Feature extraction involves transforming raw text data into numerical features that can be used for analysis and modeling. Common techniques include:\n",
        "\n",
        "* Bag of Words (BoW): Representing text as a matrix of word frequencies or presence/absence indicators.\n",
        "* Term Frequency-Inverse Document Frequency (TF-IDF): Assigning weights to words based on their frequency in a document relative to their frequency across the entire corpus.\n",
        "* Word Embeddings: Representing words as dense vectors in a continuous vector space, capturing semantic relationships between words.\n",
        "\n",
        "### Key NLP Tasks and Algorithms:\n",
        "\n",
        "NLP comprises various tasks, each with its specific goal and set of algorithms:\n",
        "\n",
        "* Sentiment Analysis: Determining the sentiment or emotion expressed in a piece of text. Algorithms such as Na√Øve Bayes, Support Vector Machines (SVM), and Recurrent Neural Networks (RNNs) are commonly used.\n",
        "* Text Classification: Categorizing text into predefined classes or categories. Algorithms like Decision Trees, Random Forests, and Neural Networks are employed for text classification tasks.\n",
        "* Named Entity Recognition (NER): Identifying and classifying named entities such as names, organizations, dates, and locations in text. Conditional Random Fields (CRFs) and Bidirectional LSTMs are popular for NER tasks.\n",
        "* Machine Translation: Translating text from one language to another. Statistical methods, rule-based approaches, and sequence-to-sequence models with attention mechanisms are used for machine translation.\n",
        "\n",
        "### Advanced NLP Techniques:\n",
        "\n",
        "* Part-of-Speech (POS) Tagging: Assigning grammatical tags to words in a sentence.\n",
        "* Dependency Parsing: Analyzing the grammatical structure and relationships between words in a sentence.\n",
        "* Coreference Resolution: Identifying and resolving references to the same entity across text.\n",
        "* Topic Modeling: Identifying themes or topics in a collection of documents using techniques like Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF).\n",
        "* Sequence-to-Sequence Models: Generating sequences of text, such as machine translation or text summarization, using models like Encoder-Decoder architectures with attention mechanisms.\n",
        "\n",
        "### Deep Learning for NLP:\n",
        "\n",
        "Deep learning has significantly advanced NLP by providing powerful models capable of learning complex patterns and representations from data. Key deep learning architectures for NLP include:\n",
        "\n",
        "* Recurrent Neural Networks (RNNs): Suitable for sequential data processing tasks due to their ability to capture temporal dependencies.\n",
        "* Long Short-Term Memory (LSTM) Networks: A type of RNN that addresses the vanishing gradient problem, enabling better learning of long-range dependencies.\n",
        "* Transformer Models: Introduced by the \"Attention is All You Need\" paper, transformer models like BERT, GPT, and XLNet have achieved state-of-the-art performance on various NLP tasks by leveraging self-attention mechanisms.\n",
        "\n",
        "### NLP Libraries and Tools:\n",
        "\n",
        "* NLTK (Natural Language Toolkit): A comprehensive library for NLP tasks in Python, providing tools for tokenization, stemming, lemmatization, POS tagging, and more.\n",
        "* SpaCy: An open-source NLP library that offers efficient tokenization, POS tagging, dependency parsing, and named entity recognition, optimized for production use.\n",
        "* Gensim: Gensim is a Python library designed for topic modeling and document similarity analysis. It specializes in unsupervised learning algorithms for semantic analysis of text data. Gensim offers implementations of popular algorithms such as Latent Semantic Analysis (LSA), Latent Dirichlet Allocation (LDA), and Word2Vec. It is widely used for tasks such as document clustering, semantic indexing, and similarity retrieval.\n",
        "* TextBlob: TextBlob is a simplified and beginner-friendly NLP library built on top of NLTK and Pattern libraries. It provides a simple API for common NLP tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\n",
        "* AllenNLP: AllenNLP is a powerful open-source NLP library built on top of PyTorch, designed for research and production-level NLP applications. It provides pre-built models and tools for various NLP tasks such as text classification, named entity recognition, semantic role labeling, and coreference resolution.\n",
        "* Stanford NLP: The Stanford NLP toolkit is a suite of NLP tools developed by the Stanford NLP Group. It provides robust and efficient implementations of state-of-the-art NLP algorithms for tasks such as part-of-speech tagging, named entity recognition, dependency parsing, sentiment analysis, and coreference resolution.\n",
        "* Transformers (Hugging Face): Transformers is a popular library developed by Hugging Face that provides pre-trained models and utilities for working with transformer-based architectures in NLP. It offers a wide range of pre-trained models such as BERT, GPT, RoBERTa, and T5, which can be fine-tuned for specific downstream tasks such as text classification, question answering, summarization, and translation.\n",
        "* TensorFlow Text: TensorFlow Text is a library built on top of TensorFlow for text processing and sequence modeling tasks. It provides modules for tokenization, preprocessing, and feature extraction, as well as implementations of popular NLP algorithms such as Word2Vec, TF-IDF, and sequence-to-sequence models. TensorFlow Text integrates seamlessly with other TensorFlow components, allowing for efficient development and deployment of end-to-end NLP pipelines.\n",
        "* FastText: FastText is a library developed by Facebook Research for efficient text classification and word representation learning. It offers implementations of fast and scalable algorithms for training word embeddings and text classifiers. FastText is known for its ability to handle large text corpora and perform well on tasks such as sentiment analysis, topic classification, and language identification.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Natural Language Processing (NLP) has emerged as a transformative field within artificial intelligence, enabling machines to understand, interpret, and generate human language. With the rapid advancement of technology and the availability of powerful NLP libraries and tools, developers and researchers can now tackle a wide range of linguistic tasks with unprecedented accuracy and efficiency.\n",
        "\n",
        "In this comprehensive overview, we explored the fundamental concepts, key components, and practical applications of NLP. We delved into the history of NLP, tracing its evolution from rule-based systems to modern machine learning approaches. We discussed essential NLP tasks such as tokenization, part-of-speech tagging, named entity recognition, sentiment analysis, and machine translation, highlighting their significance in various domains.\n",
        "\n",
        "Furthermore, we examined popular NLP libraries and tools such as NLTK, SpaCy, Gensim, TextBlob, AllenNLP, Stanford NLP, Transformers (Hugging Face), TensorFlow Text, and FastText, each offering unique functionalities and capabilities for text processing and analysis. These libraries empower developers to build sophisticated NLP pipelines and applications with ease, leveraging pre-trained models and state-of-the-art algorithms.\n",
        "\n",
        "As NLP continues to evolve, driven by advancements in deep learning, transfer learning, and natural language understanding, its impact on society, business, and academia will only grow stronger. From chatbots and virtual assistants to sentiment analysis and language translation, NLP technologies are reshaping how humans interact with machines and each other, unlocking new opportunities for communication, innovation, and discovery.\n",
        "\n",
        "In conclusion, NLP represents a cornerstone of modern artificial intelligence, bridging the gap between human language and machine intelligence. By harnessing the power of NLP, we can unlock the full potential of textual data, enabling machines to comprehend, analyze, and generate natural language with unprecedented accuracy and sophistication. As we continue to push the boundaries of NLP research and development, the future holds immense promise for the transformative impact of natural language processing on society and beyond."
      ],
      "metadata": {
        "id": "uOIxQiv-iLdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center> <h1> WordNet Access </h1> </center>"
      ],
      "metadata": {
        "id": "2IY1ikY7ecdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "synsets= wordnet.synsets('car')\n",
        "\n",
        "for synset in synsets:\n",
        "  print(synset.definition())\n",
        "\n",
        "print(wordnet.synset('car.n.01').examples())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tn0jl6h4eF1Y",
        "outputId": "88de109a-b257-47fc-a084-ee1e3269338c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
            "a wheeled vehicle adapted to the rails of railroad\n",
            "the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant\n",
            "where passengers ride up and down\n",
            "a conveyance for passengers or freight on a cable railway\n",
            "['he needs a car to get to work']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "synsets= wordnet.synsets('Apple')\n",
        "\n",
        "for synset in synsets:\n",
        "  print(synset.definition())\n",
        "\n",
        "print(wordnet.synset('Apple.n.01').examples())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddtYVX1DgAgU",
        "outputId": "5eb3b75a-599c-41f4-b98c-d63aa468fc54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fruit with red or yellow or green skin and sweet to tart crisp whitish flesh\n",
            "native Eurasian tree widely cultivated in many varieties for its firm rounded edible fruits\n",
            "[]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center> <h1> Brown Corpus </h1> </center>"
      ],
      "metadata": {
        "id": "Op6Ghft8fdgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "nltk.download('brown')\n",
        "\n",
        "categories= brown.categories()\n",
        "\n",
        "words= brown.words(categories= 'news')\n",
        "print(words[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAJ_58UtfkwF",
        "outputId": "c280afcf-1a73-486b-be5c-6f1235ec825c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.', 'The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "categories= brown.categories()\n",
        "\n",
        "words= brown.words(categories= 'fiction')\n",
        "print(words[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jl3MOMWsg9RX",
        "outputId": "72d184b9-26b5-43c1-acac-08203a8cae01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Thirty-three', 'Scotty', 'did', 'not', 'go', 'back', 'to', 'school', '.', 'His', 'parents', 'talked', 'seriously', 'and', 'lengthily', 'to', 'their', 'own', 'doctor', 'and', 'to', 'a', 'specialist', 'at', 'the', 'University', 'Hospital', '--', 'Mr.', 'McKinley', 'was', 'entitled', 'to', 'a', 'discount', 'for', 'members', 'of', 'his', 'family', '--', 'and', 'it', 'was', 'decided', 'it', 'would', 'be', 'best', 'for']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center> <h1> Movie Reviews </h1> </center>"
      ],
      "metadata": {
        "id": "NZYObo4QgjKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# from nltk.corpus import wordnet\n",
        "\n",
        "# Download the movie_reviews corpus\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "# Access categories (sentiments) in the Movie Reviews Corpus\n",
        "categories = movie_reviews.categories()\n",
        "\n",
        "# Access fileids (individual movie reviews) in a specific category\n",
        "fileids_positive = movie_reviews.fileids(categories= 'pos')\n",
        "fileids_negative = movie_reviews.fileids(categories= 'neg')\n",
        "\n",
        "# Access words in a specific movie review\n",
        "words_positie=  movie_reviews.words(fileids= fileids_positive)\n",
        "words_negative=  movie_reviews.words(fileids= fileids_negative)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lThmvmDgeZB",
        "outputId": "fab30dac-469b-4e44-c5eb-b8c238c10cdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_positie"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gk8qpnFh04c",
        "outputId": "16303843-aa60-4dcf-b1bb-dd6e70d2ba94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['films', 'adapted', 'from', 'comic', 'books', 'have', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_negative"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuZHPHYoh350",
        "outputId": "28eae815-4704-4396-e271-99716414e4f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Frequency distribution in NLP\n",
        "\n",
        "### Introduction:\n",
        "\n",
        "Natural Language Processing (NLP) stands at the convergence of computer science, artificial intelligence, and linguistics, aiming to equip machines with the capability to understand, interpret, and generate human language. One of the fundamental concepts in NLP is frequency distribution, a cornerstone for various linguistic analyses such as text analysis, sentiment analysis, and topic modeling. Frequency distribution provides insights into the distribution of words within a text or corpus, shedding light on the underlying structure and patterns of language usage.\n",
        "\n",
        "### Understanding Frequency Distribution:\n",
        "\n",
        "Frequency distribution in NLP refers to the statistical representation of how often each word occurs within a given text or corpus. It quantifies the occurrences of words, phrases, or other textual elements, providing a quantitative understanding of language usage. By analyzing frequency distributions, researchers can uncover key insights such as the most common words, thematic concentrations, and stylistic preferences within a text.\n",
        "\n",
        "The formula for calculating the frequency of a word $w$ in a text or corpus is:\n",
        "\n",
        "$$\\text{Frequency(w)=Total number of words in the text/corpusNumber of occurrences of w} \\times 100$$\n",
        "\n",
        "### Applications of Frequency Distribution:\n",
        "\n",
        "1. Text Analysis: Frequency distribution is crucial for understanding the thematic focus and vocabulary usage within a text. By identifying frequently occurring words, researchers can discern central topics and analyze the linguistic characteristics of the text.\n",
        "2. Vocabulary Building: In language learning applications, frequency distribution is utilized to construct vocabulary lists, prioritizing commonly used words for learners to acquire first.\n",
        "3. Data Preprocessing for Machine Learning: In machine learning tasks involving text data, frequency distribution is employed for data preprocessing. Commonly occurring words, known as stopwords, are often filtered out to improve the quality of analysis.\n",
        "4. Sentiment Analysis: Frequency distributions of words can indicate the sentiment or emotional tone of a text. By analyzing the frequency of positive and negative words, sentiment analysis algorithms can infer the overall sentiment of a document.\n",
        "\n",
        "### Techniques for Analyzing Frequency Distribution:\n",
        "\n",
        "1. Frequency Distribution Tables: These tables list words along with their corresponding frequencies, providing a clear and concise summary of word occurrences within a text.\n",
        "2. Histograms: Histograms visualize frequency distributions by plotting words or phrases on the x-axis and their frequencies on the y-axis. This graphical representation allows for a visual assessment of word frequency distributions.\n",
        "3. Word Clouds: Word clouds visually represent word frequencies by displaying words in varying sizes based on their frequency. This technique offers an intuitive way to identify prominent themes or topics within a text.\n",
        "\n",
        "### Challenges in Analyzing Frequency Distribution:\n",
        "\n",
        "1. High Frequency but Low Information Words: Common words such as articles and prepositions often have high frequencies but may carry little semantic meaning. Analyzing frequency distributions requires consideration of both frequency and informativeness.\n",
        "2. Polysemy and Homonymy: Words with multiple meanings (polysemy) or words that sound alike but have different meanings (homonymy) can introduce ambiguity in frequency distributions, requiring context-aware analysis.\n",
        "3. Language and Context Dependence: The significance of word frequencies can vary across different languages and contexts. Analyzing frequency distributions necessitates understanding the specific linguistic and contextual factors at play.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "Frequency distribution serves as a cornerstone in NLP, enabling researchers to gain valuable insights into language usage and textual characteristics. By analyzing the frequency of words, phrases, and other textual elements, researchers can uncover patterns, themes, and stylistic preferences within a text or corpus. Despite its simplicity, analyzing frequency distributions requires careful consideration of linguistic nuances, context, and potential challenges such as polysemy and homonymy. As NLP continues to advance, frequency distribution analysis will remain a fundamental technique for understanding and interpreting human language."
      ],
      "metadata": {
        "id": "2sXdBC38lqzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detailed Exploration on Tokenizers and its Types\n",
        "\n",
        "Tokenization is the process of breaking down a text into smaller units called tokens, which can be words, phrases, symbols, or other meaningful elements. Tokenization is a crucial step in natural language processing (NLP) tasks as it forms the foundation for further analysis and processing of textual data. There are several types of tokenizers used in NLP, each with its own characteristics and applications.  \n",
        "\n",
        "### Word Tokenization:\n",
        "\n",
        "Definition: Word tokenization is a fundamental process in natural language processing (NLP) that involves breaking down a text into individual words or tokens. It forms the initial step in analyzing textual data and serves as the foundation for various NLP tasks.\n",
        "\n",
        "#### Techniques:\n",
        "\n",
        "1. Whitespace Tokenization:\n",
        "\n",
        "* Description: Whitespace tokenization splits the text based on whitespace characters such as spaces, tabs, and newline characters.\n",
        "* Implementation: It involves scanning the text and identifying whitespace characters as token boundaries.\n",
        "* Example: Given the text \"The quick brown fox jumps over the lazy dog,\" whitespace tokenization would produce the tokens: [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"].\n",
        "* Advantages: Simple and efficient approach suitable for many languages and text formats.\n",
        "\n",
        "2. Punctuation Tokenization:\n",
        "\n",
        "* Description: Punctuation tokenization divides the text based on punctuation marks such as periods, commas, and hyphens.\n",
        "* Implementation: It identifies punctuation marks as token boundaries while preserving them as separate tokens.\n",
        "* Example: For the text \"Hello, world! How are you?\", punctuation tokenization would generate the tokens: [\"Hello\", \",\", \"world\", \"!\", \"How\", \"are\", \"you\", \"?\"].\n",
        "* Advantages: Useful for tasks where punctuation marks carry semantic importance, such as sentiment analysis or parsing.\n",
        "\n",
        "3. Language-Specific Tokenization:\n",
        "\n",
        "* Description: Some languages, like Chinese and Japanese, lack spaces between words, making tokenization more challenging.\n",
        "* Implementation: Specialized tokenization algorithms are required to handle these languages effectively, often based on linguistic rules or statistical models.\n",
        "* Example: In Chinese, a sentence like \"ÊàëÁà±Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ\" (meaning \"I love natural language processing\") would be tokenized as [\"Êàë\", \"Áà±\", \"Ëá™ÁÑ∂\", \"ËØ≠Ë®Ä\", \"Â§ÑÁêÜ\"].\n",
        "* Advantages: Ensures accurate tokenization for languages with unique writing systems or word segmentation conventions.\n",
        "\n",
        "**Applications:** Word tokenization finds application across various NLP tasks, including:\n",
        "\n",
        "* Text Classification: Breaking down text into words facilitates feature extraction and modeling for tasks such as sentiment analysis, spam detection, and topic classification.\n",
        "* Named Entity Recognition (NER): Identifying individual words allows for the recognition and tagging of named entities such as person names, locations, and organizations.\n",
        "* Language Modeling: Word tokenization is essential for training language models to predict the probability of word sequences, enabling tasks like speech recognition, machine translation, and autocomplete.\n",
        "\n",
        "### Sentence Tokenization:\n",
        "\n",
        "Definition: Sentence tokenization, also known as sentence segmentation, is the process of dividing a text into individual sentences. Each sentence typically represents a complete thought or unit of meaning within the text. Sentence tokenization is essential for various natural language processing (NLP) tasks that require sentence-level analysis or processing.\n",
        "\n",
        "#### Common Techniques:\n",
        "\n",
        "1. Rule-based Tokenization:\n",
        "\n",
        "* Description: Rule-based tokenization employs predefined rules to identify sentence boundaries based on punctuation marks and other linguistic cues.\n",
        "* Implementation: This approach typically involves scanning the text for punctuation marks that indicate the end of a sentence, such as periods (.), question marks (?), and exclamation marks (!). The presence of these punctuation marks, along with contextual rules, helps determine sentence boundaries.\n",
        "* Example: For the text \"The quick brown fox. Jumps over the lazy dog!\", rule-based tokenization would identify two sentences: \"The quick brown fox.\" and \"Jumps over the lazy dog!\".\n",
        "* Advantages: Rule-based tokenization is straightforward to implement and can handle most common cases effectively.\n",
        "\n",
        "2. Machine Learning-based Tokenization:\n",
        "\n",
        "* Description: Machine learning-based tokenization utilizes machine learning models trained on large corpora to predict sentence boundaries based on patterns observed in the text.\n",
        "* Implementation: This approach involves training a machine learning model, such as a classifier or sequence labeling model, on annotated data where sentence boundaries are marked. The trained model then predicts sentence boundaries for unseen text based on learned patterns.\n",
        "* Example: An LSTM (Long Short-Term Memory) model trained on a corpus of text with annotated sentence boundaries can be used to predict sentence boundaries for new text inputs.\n",
        "* Advantages: Machine learning-based tokenization can capture complex patterns and nuances in sentence structure, making it suitable for handling diverse text data.\n",
        "\n",
        "**Applications:** Sentence tokenization is widely used in various NLP applications, including:\n",
        "\n",
        "* Text Summarization: Sentence tokenization enables the identification of individual sentences for summarization, where key sentences are extracted to generate a concise summary of the text.\n",
        "* Machine Translation: Sentence tokenization facilitates the division of source and target language texts into individual sentences, which are then translated independently.\n",
        "* Named Entity Recognition (NER): In NER tasks, sentence tokenization helps identify boundaries for named entities, such as person names, locations, and organizations, within sentences.\n",
        "* Information Extraction: Sentence tokenization assists in extracting structured information from text by segmenting the text into meaningful units for further analysis.\n",
        "\n",
        "### Subword Tokenization:\n",
        "\n",
        "Definition: Subword tokenization is a technique in natural language processing (NLP) that breaks down a text into smaller units, which can be parts of words or complete words. Unlike word tokenization, which divides text solely based on word boundaries, subword tokenization operates at a more granular level, allowing for the representation of morphologically rich languages and handling out-of-vocabulary words more effectively.\n",
        "\n",
        "#### Techniques:\n",
        "\n",
        "1. Byte Pair Encoding (BPE):\n",
        "\n",
        "* Description: Byte Pair Encoding is a subword tokenization algorithm that merges the most frequent pairs of characters or character sequences iteratively to create a vocabulary of subword units.\n",
        "* Implementation:\n",
        "* Example: In the word \"subword,\" the pairs \"su,\" \"bw,\" and \"or\" might be merged iteratively to form the subword units [\"sub\", \"wo\", \"rd\"].\n",
        "* Advantages: BPE can capture both frequent words and rare morphological variations effectively, making it suitable for a wide range of languages and tasks.\n",
        "\n",
        "2. WordPiece Tokenization:\n",
        "\n",
        "* Description: WordPiece tokenization is a variant of BPE that also merges characters or character sequences iteratively but uses a different merging strategy to create subword units.\n",
        "* Implementation:\n",
        "* Example: In the word \"subword,\" WordPiece tokenization might merge characters or sequences based on statistical measures or language-specific considerations to form subword units.\n",
        "* Advantages: WordPiece tokenization may yield more optimized vocabularies compared to BPE in some cases, especially when considering linguistic characteristics or specific task requirements.\n",
        "\n",
        "**Applications:** Subword tokenization is particularly useful in various NLP tasks and scenarios, including:\n",
        "\n",
        "* Handling Morphologically Rich Languages: Subword tokenization is effective for languages with complex morphology, where words can be composed of multiple morphemes or affixes.\n",
        "* Out-of-Vocabulary Words: Subword tokenization helps address the issue of out-of-vocabulary words by representing rare or unseen words as combinations of subword units.\n",
        "* Machine Translation: Subword tokenization enables more robust translation models by providing a finer-grained representation of source and target languages, especially for languages with agglutinative or fusional morphology.\n",
        "* Speech Recognition: Subword tokenization assists in building language models for speech recognition systems, where the vocabulary size may be large and handling out-of-vocabulary words is critical for accuracy.\n",
        "\n",
        "### Regular Expression Tokenization:\n",
        "\n",
        "Definition: Regular expression tokenization is a tokenization technique that involves using regular expressions to define patterns for identifying tokens within a text. Regular expressions provide a powerful and flexible way to specify patterns of characters, enabling tokenization based on custom-defined criteria.\n",
        "\n",
        "#### Key Features:\n",
        "\n",
        "* Flexibility: Regular expressions offer flexibility in defining tokenization patterns, allowing for customization to meet specific requirements.\n",
        "* Customization: Tokenization rules can be tailored to handle various types of tokens, such as email addresses, URLs, or specific formatting conventions.\n",
        "* Pattern Matching: Regular expressions use pattern matching to identify tokens within the text, enabling precise tokenization based on user-defined patterns.\n",
        "\n",
        "#### Example Applications:\n",
        "\n",
        "* Email Addresses: Regular expressions can be used to tokenize text and identify email addresses within the text. For example, the pattern \"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+.[A-Z|a-z]{2,}\\b\" matches typical email address formats.\n",
        "* URLs: Regular expressions can tokenize text to identify URLs or web addresses. For example, the pattern \"(http|https)://[^\\s]+\" matches URLs starting with \"http://\" or \"https://\".\n",
        "* Formatting Conventions: Regular expressions can tokenize text based on specific formatting conventions, such as dates, phone numbers, or numerical values.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* Customizability: Regular expression tokenization can be customized to handle specific tokenization requirements and patterns.\n",
        "* Versatility: Regular expressions can be applied to various types of text data and tokenization tasks.\n",
        "* Efficiency: Regular expression tokenization can efficiently identify tokens based on predefined patterns, even in large text corpora.\n",
        "\n",
        "### Tokenizer for Specialized Domains:\n",
        "\n",
        "Definition: Tokenizer for specialized domains refers to tokenization techniques specifically designed to handle the unique characteristics and terminology of specialized domains, such as biomedical text or legal documents. These tokenizers incorporate domain-specific rules, dictionaries, or machine learning models trained on domain-specific data to achieve accurate tokenization.\n",
        "\n",
        "#### Key Features:\n",
        "\n",
        "* Domain-specific Rules: Tokenizers for specialized domains may incorporate rules tailored to handle the unique linguistic characteristics and terminology of the domain.\n",
        "* Dictionaries: Domain-specific tokenizers may utilize dictionaries or lexicons containing domain-specific terms and vocabulary to assist in tokenization.\n",
        "* Machine Learning Models: Some tokenizers may leverage machine learning models trained on domain-specific data to improve tokenization accuracy and performance.\n",
        "\n",
        "#### Example Applications:\n",
        "\n",
        "* Biomedical Text: Tokenizers for biomedical text may handle specialized terminology, abbreviations, and symbols commonly found in medical literature.\n",
        "* Legal Documents: Tokenizers for legal documents may address the unique formatting conventions, legal terminology, and citation styles prevalent in legal texts.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* Accuracy: Tokenizers for specialized domains can achieve higher accuracy by considering domain-specific rules and terminology.\n",
        "* Improved Performance: By incorporating domain-specific knowledge, these tokenizers can improve performance in tokenizing text from specialized domains.\n",
        "* Customizability: Tokenizers for specialized domains can be customized to handle specific linguistic features and requirements of the domain.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Tokenization is a fundamental preprocessing step in natural language processing tasks, and the choice of tokenizer depends on the specific requirements of the task and the characteristics of the text data. Each type of tokenizer has its advantages and limitations, and selecting the appropriate tokenizer is essential for accurate and effective analysis of textual data in NLP applications."
      ],
      "metadata": {
        "id": "oMPRugRxmgsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center> <h1> Bi-gram, Tri-gram, N-gram </h1> </center>"
      ],
      "metadata": {
        "id": "jbiepPO_o-Mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import bigrams, ngrams, trigrams\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "string= \"Natural language processing (NLP) is a machine learning technology that gives computers the ability to interpret, manipulate, and comprehend human language.\"\n",
        "\n",
        "token= nltk.word_tokenize(string)\n",
        "token"
      ],
      "metadata": {
        "id": "lSqxxvGJh7-x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7a2acd0-74c8-4fd9-ed45-c52abf02744d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " '(',\n",
              " 'NLP',\n",
              " ')',\n",
              " 'is',\n",
              " 'a',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'technology',\n",
              " 'that',\n",
              " 'gives',\n",
              " 'computers',\n",
              " 'the',\n",
              " 'ability',\n",
              " 'to',\n",
              " 'interpret',\n",
              " ',',\n",
              " 'manipulate',\n",
              " ',',\n",
              " 'and',\n",
              " 'comprehend',\n",
              " 'human',\n",
              " 'language',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram= list(bigrams(token))\n",
        "bigram"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbqu22InpraF",
        "outputId": "1de82fb1-8cd6-4c76-9414-16e72bb7dde4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Natural', 'language'),\n",
              " ('language', 'processing'),\n",
              " ('processing', '('),\n",
              " ('(', 'NLP'),\n",
              " ('NLP', ')'),\n",
              " (')', 'is'),\n",
              " ('is', 'a'),\n",
              " ('a', 'machine'),\n",
              " ('machine', 'learning'),\n",
              " ('learning', 'technology'),\n",
              " ('technology', 'that'),\n",
              " ('that', 'gives'),\n",
              " ('gives', 'computers'),\n",
              " ('computers', 'the'),\n",
              " ('the', 'ability'),\n",
              " ('ability', 'to'),\n",
              " ('to', 'interpret'),\n",
              " ('interpret', ','),\n",
              " (',', 'manipulate'),\n",
              " ('manipulate', ','),\n",
              " (',', 'and'),\n",
              " ('and', 'comprehend'),\n",
              " ('comprehend', 'human'),\n",
              " ('human', 'language'),\n",
              " ('language', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trigram= list(trigrams(token))\n",
        "trigram"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-gf8JQbrA6a",
        "outputId": "27d9dce4-46f8-4306-f94c-dd930c6e2d49"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Natural', 'language', 'processing'),\n",
              " ('language', 'processing', '('),\n",
              " ('processing', '(', 'NLP'),\n",
              " ('(', 'NLP', ')'),\n",
              " ('NLP', ')', 'is'),\n",
              " (')', 'is', 'a'),\n",
              " ('is', 'a', 'machine'),\n",
              " ('a', 'machine', 'learning'),\n",
              " ('machine', 'learning', 'technology'),\n",
              " ('learning', 'technology', 'that'),\n",
              " ('technology', 'that', 'gives'),\n",
              " ('that', 'gives', 'computers'),\n",
              " ('gives', 'computers', 'the'),\n",
              " ('computers', 'the', 'ability'),\n",
              " ('the', 'ability', 'to'),\n",
              " ('ability', 'to', 'interpret'),\n",
              " ('to', 'interpret', ','),\n",
              " ('interpret', ',', 'manipulate'),\n",
              " (',', 'manipulate', ','),\n",
              " ('manipulate', ',', 'and'),\n",
              " (',', 'and', 'comprehend'),\n",
              " ('and', 'comprehend', 'human'),\n",
              " ('comprehend', 'human', 'language'),\n",
              " ('human', 'language', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_gram= list(ngrams(token, n= 4))\n",
        "n_gram"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8N2oA3DrWdd",
        "outputId": "d4b88068-3294-4c90-e556-9abee584b8c1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Natural', 'language', 'processing', '('),\n",
              " ('language', 'processing', '(', 'NLP'),\n",
              " ('processing', '(', 'NLP', ')'),\n",
              " ('(', 'NLP', ')', 'is'),\n",
              " ('NLP', ')', 'is', 'a'),\n",
              " (')', 'is', 'a', 'machine'),\n",
              " ('is', 'a', 'machine', 'learning'),\n",
              " ('a', 'machine', 'learning', 'technology'),\n",
              " ('machine', 'learning', 'technology', 'that'),\n",
              " ('learning', 'technology', 'that', 'gives'),\n",
              " ('technology', 'that', 'gives', 'computers'),\n",
              " ('that', 'gives', 'computers', 'the'),\n",
              " ('gives', 'computers', 'the', 'ability'),\n",
              " ('computers', 'the', 'ability', 'to'),\n",
              " ('the', 'ability', 'to', 'interpret'),\n",
              " ('ability', 'to', 'interpret', ','),\n",
              " ('to', 'interpret', ',', 'manipulate'),\n",
              " ('interpret', ',', 'manipulate', ','),\n",
              " (',', 'manipulate', ',', 'and'),\n",
              " ('manipulate', ',', 'and', 'comprehend'),\n",
              " (',', 'and', 'comprehend', 'human'),\n",
              " ('and', 'comprehend', 'human', 'language'),\n",
              " ('comprehend', 'human', 'language', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center> <h1> Stemming </h1> </center>"
      ],
      "metadata": {
        "id": "Tr_bu0Nyr3Rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "# Initializing the Porter Stemer\n",
        "pst = PorterStemmer()\n",
        "# steming a single word \"having\"\n",
        "print(\"Stemming 'having' :\", pst.stem('having'))\n",
        "\n",
        "word_to_stem= ['give', 'giving', 'given', 'gave']\n",
        "for word in word_to_stem:\n",
        "  print(word + \" : \"+ pst.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r234Tz6rk3W",
        "outputId": "47d93f93-67e7-4826-eb26-a02ed9229765"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming 'having' : have\n",
            "give : give\n",
            "giving : give\n",
            "given : given\n",
            "gave : gave\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "# Initializing the Porter Stemer\n",
        "lst = LancasterStemmer()\n",
        "# steming a single word \"having\"\n",
        "print(\"Stemming 'having' :\", lst.stem('having'))\n",
        "\n",
        "word_to_stem= ['give', 'giving', 'given', 'gave']\n",
        "for word in word_to_stem:\n",
        "  print(word + \" : \"+ lst.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-12nyi5uHVV",
        "outputId": "31e214c0-a820-4906-8f23-c1460220d900"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming 'having' : hav\n",
            "give : giv\n",
            "giving : giv\n",
            "given : giv\n",
            "gave : gav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "sbst= SnowballStemmer('english')\n",
        "\n",
        "support_language= sbst.languages\n",
        "print(support_language)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFGszurGugLM",
        "outputId": "e0555be3-9a7e-413d-bff1-dcc3f5cabd9c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "sbst= SnowballStemmer('english')\n",
        "\n",
        "print(\"Stemming 'having' :\", sbst.stem('having'))\n",
        "\n",
        "word_to_stem= ['give', 'giving', 'given', 'gave']\n",
        "for word in word_to_stem:\n",
        "  print(word + \" : \"+ sbst.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1GPJ9-kvLTr",
        "outputId": "46c25393-7ef7-4267-df9d-6f4e8a0062cf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming 'having' : have\n",
            "give : give\n",
            "giving : give\n",
            "given : given\n",
            "gave : gave\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center> <h1> Lemmatization </h1> </center>"
      ],
      "metadata": {
        "id": "IvFYSLXDv-BR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "word_lem= WordNetLemmatizer()\n",
        "\n",
        "print(\"Lemmatized Word 'corpora' :\", word_lem.lemmatize('corpora'))\n",
        "\n",
        "word_to_lemmat= ['give', 'giving', 'given', 'gave']\n",
        "for word in word_to_lemmat:\n",
        "  print(word + \" : \"+ word_lem.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYdqQeQQvdf8",
        "outputId": "94549723-e32f-41ea-fa77-33fbaec027b0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Word 'corpora' : corpus\n",
            "give : give\n",
            "giving : giving\n",
            "given : given\n",
            "gave : gave\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center> <h1> Stopwords </h1> </center>"
      ],
      "metadata": {
        "id": "Rm1vsK_T1F4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "nltk.download('stopwords')\n",
        "\n",
        "text= \"Natural language processing (NLP) is a machine learning technology that gives computers the ability to interpret, manipulate, and comprehend human language.\"\n",
        "\n",
        "tokens= word_tokenize(text)\n",
        "\n",
        "stop_words= set(stopwords.words('english'))\n",
        "\n",
        "filtered_tokens= [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "fdist= FreqDist(filtered_tokens)\n",
        "\n",
        "fdist_top10= fdist.most_common(10)\n",
        "\n",
        "print(\"Top 10 most common words after stopwords removal:\")\n",
        "\n",
        "print(fdist_top10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUuEKhC01EjD",
        "outputId": "43018c20-6635-4b32-b421-8406d3748c4e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 most common words after stopwords removal:\n",
            "[('language', 2), (',', 2), ('Natural', 1), ('processing', 1), ('(', 1), ('NLP', 1), (')', 1), ('machine', 1), ('learning', 1), ('technology', 1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center> <h1> Part of Speech (POS) Tagging </h1> </center>"
      ],
      "metadata": {
        "id": "rqGLZLUx3Qoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sent= \"Wasiq is Driving Luxury Car.\"\n",
        "sent_tokens= word_tokenize(sent)\n",
        "print(sent_tokens)\n",
        "\n",
        "for token in sent_tokens:\n",
        "  print(nltk.pos_tag([token]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-AJdfbyxb2J",
        "outputId": "ce50250e-8893-4fc0-bd3d-13a2af325758"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Wasiq', 'is', 'Driving', 'Luxury', 'Car', '.']\n",
            "[('Wasiq', 'NN')]\n",
            "[('is', 'VBZ')]\n",
            "[('Driving', 'VBG')]\n",
            "[('Luxury', 'NN')]\n",
            "[('Car', 'NN')]\n",
            "[('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center> <h1> Bag of Word (BOW) </h1> </center>"
      ],
      "metadata": {
        "id": "aCLhTuMf4itB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "documents = [\"This is the first document.\",\n",
        "\"This document is the second document.\",\n",
        "\"And this is the third one.\",\n",
        "\"Is this the first document?\"]\n",
        "\n",
        "vectorizer= CountVectorizer()\n",
        "\n",
        "X= vectorizer.fit_transform(documents)\n",
        "\n",
        "feature_name= vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "LhneHMra3_OS"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTVp7tgj5NZ7",
        "outputId": "1632961e-8f00-458c-8f40-978b086cf5af"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Feature Name :\", feature_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2HzBWDJ5R1L",
        "outputId": "9b3c625b-651f-4f1e-a1a6-5a49ace4279a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Name : ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(X.toarray(), columns= feature_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "-Y5tFFwv5Xxs",
        "outputId": "66bfd825-c115-4980-8a5a-1356c0a60800"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   and  document  first  is  one  second  the  third  this\n",
              "0    0         1      1   1    0       0    1      0     1\n",
              "1    0         2      0   1    0       1    1      0     1\n",
              "2    1         0      0   1    1       0    1      1     1\n",
              "3    0         1      1   1    0       0    1      0     1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d8f32e05-0e22-470d-9329-9f2e3ce77990\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>and</th>\n",
              "      <th>document</th>\n",
              "      <th>first</th>\n",
              "      <th>is</th>\n",
              "      <th>one</th>\n",
              "      <th>second</th>\n",
              "      <th>the</th>\n",
              "      <th>third</th>\n",
              "      <th>this</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8f32e05-0e22-470d-9329-9f2e3ce77990')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d8f32e05-0e22-470d-9329-9f2e3ce77990 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d8f32e05-0e22-470d-9329-9f2e3ce77990');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b0d97b39-4572-4a03-a05d-86bbc31728ac\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b0d97b39-4572-4a03-a05d-86bbc31728ac')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b0d97b39-4572-4a03-a05d-86bbc31728ac button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"and\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"document\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"first\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"is\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"one\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"second\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"the\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"third\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"this\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vcM23yoW6xHP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}