{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center> <h1> Fine Tuning </h1> </center>"
      ],
      "metadata": {
        "id": "YIiKws4dXM9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huBUIt8uk7Kn",
        "outputId": "ed0c069b-7bc0-445d-a990-f286049bebe6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.20\n",
            "  Downloading openai-0.20.0.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m984.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.20) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.20) (4.66.4)\n",
            "Requirement already satisfied: pandas>=1.2.3 in /usr/local/lib/python3.10/dist-packages (from openai==0.20) (2.0.3)\n",
            "Requirement already satisfied: pandas-stubs>=1.1.0.11 in /usr/local/lib/python3.10/dist-packages (from openai==0.20) (2.0.3.230814)\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from openai==0.20) (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl>=3.0.7->openai==0.20) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.3->openai==0.20) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.3->openai==0.20) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.3->openai==0.20) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.3->openai==0.20) (1.25.2)\n",
            "Requirement already satisfied: types-pytz>=2022.1.1 in /usr/local/lib/python3.10/dist-packages (from pandas-stubs>=1.1.0.11->openai==0.20) (2024.1.0.20240417)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.20) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.20) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.20) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.20) (2024.7.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.3->openai==0.20) (1.16.0)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.20.0-py3-none-any.whl size=54086 sha256=562508d3da31cc2c8a6a9791dcee6fa4be44851f2a4ee78e4347f8890ba1ec9a\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/bd/1d/f1d7e85562515ca1b9504a9450c89623329c0e24af7c749dc6\n",
            "Successfully built openai\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELat53hYZRZq",
        "outputId": "3c72cc48-6012-4e36-8a21-80dd6426c1dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.20.0)\n",
            "Collecting openai\n",
            "  Downloading openai-1.36.1-py3-none-any.whl (328 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.8/328.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 0.20.0\n",
            "    Uninstalling openai-0.20.0:\n",
            "      Successfully uninstalled openai-0.20.0\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.36.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "api_key= \"sk-.................\"\n",
        "openai.api_key= api_key\n",
        "\n",
        "response= openai.ChatCompletion.create(\n",
        "    model= \"gpt-3.5-turbo\",\n",
        "    messages= [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is Generative AI.\"},\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response['choice'][0]['message']['content'])"
      ],
      "metadata": {
        "id": "FsN4eZ0TYwyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "api_key= \"sk-.................\"\n",
        "openai.api_key= api_key\n",
        "\n",
        "# Initial prompt to set the tone\n",
        "messages= [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
        "\n",
        "while True:\n",
        "  # Get user input\n",
        "  user_message= input(\"You :\")\n",
        "\n",
        "  # Add user's message to conversation history\n",
        "  messages.append({'role': 'user', 'content': user_message})\n",
        "\n",
        "  # Send conversation history to OpenAI for response\n",
        "  response= openai.ChatCompletion.create(\n",
        "    model= \"gpt-3.5-turbo\",\n",
        "    messages= messages,\n",
        ")\n",
        "  # Display AI's response\n",
        "  ai_response= response['choice'][0]['message']['content']\n",
        "  print(f\"AI: {ai_response}\")\n",
        "\n",
        "  # Add AI response to conversation history\n",
        "  messages.append({\"role\": \"assistant\", \"content\": ai_response})"
      ],
      "metadata": {
        "id": "I6HV0DRmbQrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "api_key= \"sk-.................\"\n",
        "openai.api_key= api_key\n",
        "\n",
        "# Initial prompt to set the tone\n",
        "messages= [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
        "\n",
        "while True:\n",
        "  # Get user input\n",
        "  user_message= input(\"You : \")\n",
        "\n",
        "  # Add user's message to conversation history\n",
        "  messages.append({'role': 'user', 'content': user_message})\n",
        "\n",
        "  # Send conversation history to OpenAI for response\n",
        "  response= openai.ChatCompletion.create(\n",
        "    model= \"gpt-3.5-turbo\",\n",
        "    messages= messages,\n",
        "    max_tokens= 150, # Adjust based in desired response length\n",
        "    n= 1, # Change this to generatemultiple response (e.g., n=3)\n",
        "    stop= None, # Define a stop sequence if needed (e.g., stop= \"<|end|>\")\n",
        "    temperature= 0.7 # Adjust this for more creativity (higher) or safety (lower)\n",
        ")\n",
        "  # Display AI's response\n",
        "  ai_response= response['choice'][0]['message']['content']\n",
        "  print(f\"AI : {ai_response}\")\n",
        "\n",
        "  # Add AI response to conversation history\n",
        "  messages.append({\"role\": \"assistant\", \"content\": ai_response})"
      ],
      "metadata": {
        "id": "lokTvauybbm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Efficient Fine-Tuning of Parameters: Maximizing Model Performance\n",
        "\n",
        "Fine-tuning parameters is a crucial step in the development of machine learning models, especially in the realm of Natural Language Processing (NLP). Efficient fine-tuning not only enhances model performance but also optimizes computational resources and reduces training time. In this reading, we delve into the intricacies of efficient parameter fine-tuning, exploring techniques and strategies to maximize model performance while minimizing resource consumption.\n",
        "\n",
        "### Understanding Fine-Tuning\n",
        "\n",
        "Fine-tuning involves adjusting the parameters of a pre-trained model to adapt it to a specific task or domain. In the context of NLP, fine-tuning typically involves adjusting parameters such as learning rate, batch size, and optimization algorithms to optimize model performance. While fine-tuning can significantly enhance model capabilities, it often requires extensive computational resources and time-consuming training processes. Therefore, efficient fine-tuning techniques are essential to streamline this process and achieve optimal results.\n",
        "\n",
        "### Techniques for Efficient Fine-Tuning\n",
        "\n",
        "1. Transfer Learning: Transfer learning is a fundamental technique in efficient fine-tuning, allowing models to leverage knowledge gained from pre-training on large datasets. By initializing the model with pre-trained weights, fine-tuning can focus on learning task-specific features, significantly reducing training time and resource requirements.\n",
        "\n",
        "2. Gradient Accumulation: Gradient accumulation is a technique that enables training with larger batch sizes while minimizing memory requirements. Instead of updating model parameters after each batch, gradients are accumulated over multiple batches before performing a parameter update. This allows for efficient memory utilization and faster convergence during training.\n",
        "\n",
        "3. Learning Rate Scheduling: Learning rate scheduling involves dynamically adjusting the learning rate during training to optimize model performance. Techniques such as linear or exponential decay, cosine annealing, and warm-up schedules help prevent overfitting and improve convergence, leading to more efficient fine-tuning.\n",
        "\n",
        "4. Early Stopping: Early stopping is a regularization technique that monitors model performance on a validation dataset and halts training when performance begins to degrade. By preventing overfitting and unnecessary training iterations, early stopping helps improve training efficiency and prevents resource wastage.\n",
        "\n",
        "5. Gradient Clipping: Gradient clipping is a technique used to mitigate the exploding gradient problem during training. By capping the gradient magnitude, gradient clipping ensures stable training dynamics and prevents numerical instability, leading to more efficient fine-tuning.\n",
        "\n",
        "### Best Practices and Considerations\n",
        "\n",
        "Efficient fine-tuning requires careful consideration of various factors and adherence to best practices:\n",
        "\n",
        "* Hyperparameter Tuning: Fine-tuning hyperparameters such as learning rate, batch size, and optimization algorithms is essential for maximizing model performance. Experimentation and iterative tuning are key to finding optimal hyperparameter configurations.\n",
        "\n",
        "* Model Architecture Selection: Choosing the right model architecture for the task at hand is crucial for efficient fine-tuning. Consider factors such as model size, complexity, and computational requirements when selecting a model architecture.\n",
        "\n",
        "* Data Augmentation: Data augmentation techniques such as random cropping, flipping, and masking can help increase the diversity and size of the training dataset, leading to more robust and efficient fine-tuning.\n",
        "\n",
        "* Regularization: Applying regularization techniques such as dropout, weight decay, and layer normalization can help prevent overfitting and improve the generalization performance of fine-tuned models.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Efficient fine-tuning of parameters is essential for maximizing model performance while minimizing resource consumption and training time. By leveraging transfer learning, gradient accumulation, learning rate scheduling, and other techniques, practitioners can streamline the fine-tuning process and achieve optimal results in NLP tasks. Adherence to best practices and careful consideration of various factors are key to efficient fine-tuning and the development of high-performing machine learning models.\n",
        "\n"
      ],
      "metadata": {
        "id": "CdPdo8K0dQy5"
      }
    }
  ]
}